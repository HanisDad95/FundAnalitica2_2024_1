{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "pd.set_option('display.max_columns',None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.random import randint\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import *\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from  sklearn.base import clone ## Clonar un pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGUE DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_parquet(\"C:/Users/jf95n/OneDrive/Desktop/KaggleCompetition/df_train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANÁLISIS EXPLORATORIO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = full_data.loc[:,full_data.columns[full_data.isna().sum() > 0]].isna().sum().plot(kind = 'barh')\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title('Datos faltantes por variable')\n",
    "ax.set_xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por conocimiento del negocio, se tiene que los pacientes que tengan un None en la variable Multicáncer no se evidenció que tuviera otros tipos de cáncer, por lo que se realiza la imputación.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['MULTI_CANCER'].fillna(0, inplace = True)\n",
    "## CASTEAR LA VARIABLES\n",
    "full_data['MULTI_CANCER'] = pd.to_numeric(full_data['MULTI_CANCER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in full_data.isna().sum(axis = 1).value_counts().sort_index().items():\n",
    "    match i:\n",
    "        case 0: \n",
    "            print(f'En el dataset hay {j} observaciones sin datos nulos.')   \n",
    "        case _:\n",
    "            print(f'En el dataset hay {j} observaciones con datos nulos o faltantes en {i} de sus variables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = full_data['Target'].value_counts(normalize = True).plot(kind = 'bar')\n",
    "labels = (full_data.Target.value_counts(normalize = True).sort_index()*100).round(1).astype('str') + '%' \n",
    "ax.tick_params(axis = 'x', rotation = 0)\n",
    "ax.set_title(\"Categorías variable dependiente, conjunto de prueba\")\n",
    "ax.set_yticks([])\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, labels = labels)\n",
    "ax.set_title('Distribución de la variable respuesta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = full_data.groupby(['ESTADO_CIVIL','Target'])['GENERO'].count().unstack().plot(kind = 'bar')\n",
    "ax.tick_params(axis = 'x', rotation = 0)\n",
    "ax.set_title(\"Categorías variable dependiente, conjunto de prueba\")\n",
    "ax.set_title('Distribución de la variable respuesta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se define una función para obtener el índice de las variables categóricas, numéricas y variable objetivo\n",
    "def SepararNumCate(df : pd.DataFrame, target_variable : str):\n",
    "    '''Returns a triplet with column names (numerical, categorical, target)\n",
    "    '''\n",
    "    numerical = df.select_dtypes(include = 'number').columns.to_list()\n",
    "    date_time = df.select_dtypes(include = 'datetime').columns.to_list()\n",
    "    categorical = df.select_dtypes(include = 'object').columns.to_list()\n",
    "    numerical.remove(target_variable) ## REMOVES THE TARGET VARIABLE\n",
    "    target = target_variable\n",
    "    return numerical, date_time, categorical, target\n",
    "\n",
    "\n",
    "num_idx, date_time_idx, categ_idx, target_idx = SepararNumCate(full_data,'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.select_dtypes(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.loc[:,num_idx].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTLIERS UNIVARIADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTLIERS MULTIVARIADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y = full_data.loc[:,['mes_6', 'mes_5', 'mes_4', 'mes_3', 'mes_2', 'mes_1',]].agg('sum', axis = 1), x = full_data['Target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.loc[:,['GENERO','Target']].value_counts().unstack().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.loc[full_data['GENERO']=='M',['GENERO','Target']].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEPARAR 15% COMO CONJUNTO DE PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BAYESIAN OPTIMIZATION FOR IMPUTATION\n",
    "def target_imput(numerical, naDrop, nNeighborsIterator,) -> float :\n",
    "    \n",
    "\n",
    "    rf_model = RandomForestClassifier(random_state = 123)\n",
    "    nNeighborsIterator = int(nNeighborsIterator)\n",
    "    numerical = int(numerical)\n",
    "    numericalOptions = ['simple','knn']\n",
    "    data = full_data.copy()\n",
    "    naDrop = int(naDrop)\n",
    "\n",
    "    pl_dict_imputer = {\n",
    "        'simple' : SimpleImputer(strategy = 'median'),\n",
    "        'knn' :  KNNImputer(n_neighbors = nNeighborsIterator)\n",
    "                    }\n",
    "\n",
    "    num_imputer = Pipeline(steps = [\n",
    "            ('num_imputer',pl_dict_imputer[numericalOptions[numerical]]),\n",
    "            ('scaler',StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_imputer = Pipeline(steps = [\n",
    "        ('cat_imputer',SimpleImputer(strategy = 'most_frequent')),\n",
    "        ('encoder',OneHotEncoder(handle_unknown = 'ignore',drop = 'if_binary'))\n",
    "    ])    \n",
    "\n",
    "    columnImputer = ColumnTransformer(transformers = [\n",
    "        ('Numerical',num_imputer,num_idx),\n",
    "        ('Categorical', cat_imputer, categ_idx),\n",
    "                                    ]\n",
    "    )\n",
    "\n",
    "    finalPipeline = Pipeline(\n",
    "        steps = [\n",
    "            ('imputer',columnImputer),\n",
    "            ('clf',rf_model)\n",
    "                ])\n",
    "\n",
    "    match naDrop:\n",
    "        case 2:\n",
    "            dataTrain = data.drop(data.loc[data.isna().sum(axis = 1) == 2].index).copy()\n",
    "        case 4:\n",
    "            dataTrain = data.drop(data.loc[data.isna().sum(axis = 1) == 4].index).copy()\n",
    "        case 0:\n",
    "            dataTrain = data.drop(data.loc[data.isna().sum(axis = 1) == 0].index).copy()\n",
    "        case _:\n",
    "            dataTrain = data.copy()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "    \n",
    "    pipe = clone(finalPipeline)\n",
    "    return np.median(np.array(cross_val_score(pipe,dataTrain[num_idx + categ_idx], dataTrain[target_idx], cv = skf, scoring = 'f1',n_jobs = -4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pBounds_imput = dict(\n",
    "    numerical = (0,1),\n",
    "    naDrop = (0,4),\n",
    "    nNeighborsIterator = (2,50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=target_imput,\n",
    "    pbounds=pBounds_imput,\n",
    "    random_state=123,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(n_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINES DE FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AQUÍ VAN TODAS LAS VARIABLES NUEVAS QUE SE VAN A CREAR\n",
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Perform arbitary transformation\n",
    "        X_transf = X.copy()\n",
    "        X_transf['MULT_CANCER'] = X_transf['MULTI_CANCER'].fillna(0)\n",
    "        X_transf['IMC'] = X_transf['PESO']/X_transf['TALLA'] ### HACER UNA MEJOR CONSTRUCCIÓN DE ESTA VARIABLE QUE TOME EN CUENTA LA EDAD\n",
    "        X_transf['SumCosto'] = X_transf.loc[:,['mes_6', 'mes_5', 'mes_4', 'mes_3', 'mes_2', 'mes_1',]].agg(sum, axis = 1) ### \n",
    "        X_transf['EdadComplicacion'] = (((X_transf['Fecha_cero'] - X['FECHA_NACIMIENTO']).dt.components.days)/365).astype(int)        \n",
    "        X_transf.drop(labels = 'Fecha_cero', axis = 1, inplace = True) ## IMPORTANTE ESTE PASO PARA QUE NO VAYAMOS A DEJAR ESTA VARIABLE\n",
    "        return X_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPUTATION AND STANDARDIZATION PIPELINES =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC(kernel, gamma, C, degree, coef0, tol):\n",
    "    kernel = int(kernel) ## (0,3)\n",
    "    n_jobs = -3 ## to use all but 2 cores.\n",
    "    kernels = ['linear','polynomial','rbf','sigmoid']\n",
    "    # match kernel:\n",
    "    #     case 2:\n",
    "    #         # use gamma : must be non-negative\n",
    "    #     case _:\n",
    "    #         # dont use gamma\n",
    "    classificator = SVC(C = C, kernel = kernels[kernel], class_weight = )\n",
    "    return -np.median(cross_val_score(classificator, x_train, y_train, n_jobs = n_jobs, cv = 5, scoring = 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION WITH LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCreation_Cleaning(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Perform arbitary transformation\n",
    "        X_transf = X.copy()\n",
    "        # X_transf = X_transf.drop(X_transf.loc[X_transf.isna().sum(axis = 1) == 4].index).copy()\n",
    "        X_transf.drop(labels = ['Fecha_cero'], axis = 1, inplace = True) ## IMPORTANTE ESTE PASO PARA QUE NO VAYAMOS A DEJAR ESTA VARIABLE\n",
    "        return X_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SE DEBE ELIMINAR LAS FILAS QUE TENGAN 4 FALTANTES ASÍ O MIRAR COMO SE ACOMODA EL FEATURE TRANSFORMER PORQUE SÓLO ELIMINA LAS FILAS EN LAS X's Y NO EN LAS Y's.\n",
    "full_data_2 = full_data.drop(full_data.loc[full_data.isna().sum(axis = 1) == 4].index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class LassoFeatureSelectorCV(TransformerMixin):\n",
    "    def __init__(self, n_jobs = -4, cv = skf):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.cv = cv\n",
    "        self.model = LassoCV(cv = self.cv, n_jobs = self.n_jobs)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        self.important_features_ = self.model.coef_ != 0\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.important_features_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PCA\n",
    "canc_transf = FeatureCreation_Cleaning()\n",
    "imputer_knn =   KNNImputer(n_neighbors = 35)\n",
    "scaler_pca = StandardScaler()\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "princ_comp = PCA()\n",
    "xgb_clf = XGBClassifier(scale_pos_weight = )\n",
    "\n",
    "numer_prepro = Pipeline( steps = [\n",
    "        ('imputer',imputer_knn),\n",
    "        ('scaler',scaler_pca),\n",
    "        ('dim_reduc',princ_comp),\n",
    "        ('feat_selec', LassoFeatureSelectorCV(cv = skf))\n",
    "])\n",
    "\n",
    "categ_prepro = Pipeline( steps = [\n",
    "        ('cat_imputer',SimpleImputer(strategy = 'most_frequent')),\n",
    "        ('encoder',OneHotEncoder(handle_unknown = 'ignore',drop = 'if_binary'))\n",
    "])\n",
    "\n",
    "column_prepro = ColumnTransformer( transformers = [\n",
    "        ('num',numer_prepro, num_idx),\n",
    "        ('categ',categ_prepro, categ_idx)\n",
    "])\n",
    "\n",
    "final_pipeline = Pipeline( steps = [\n",
    "        ('Preprocessing',column_prepro),\n",
    "        ('clf',xgb_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBClf_bayes_opt(learning_rate, n_estimators, max_depth, subsample, colsample,\n",
    "                    reg_alpha, reg_lambda,scale_pw):\n",
    "                    \n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "    full_data2 = full_data_2.copy()\n",
    "    n_jobs = -4\n",
    "    xgb_grid_params = {'clf__learning_rate': learning_rate,\n",
    "                       'clf__subsample'    : subsample,\n",
    "                       'clf__n_estimators' :  int(n_estimators), \n",
    "                       'clf__reg_alpha': reg_alpha,\n",
    "                       'clf__reg_lambda': reg_lambda ,\n",
    "                       'clf__scale_pos_weight':scale_pw,\n",
    "                       'clf__max_depth'    : int(max_depth) }\n",
    "    clf = clone(final_pipeline.set_params(**xgb_grid_params))\n",
    "    return np.median(cross_val_score(clf,full_data2.loc[:,num_idx + categ_idx],full_data2.loc[:,target_idx], cv = skf, scoring = 'f1', n_jobs = n_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds_XGB = {\n",
    "    'learning_rate': (0.001, 1.0),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'scale_pw':(0,12),\n",
    "    'max_depth': (3,20),\n",
    "    'subsample': (0.2, 1.0),  # Change for big datasets\n",
    "    'colsample': (0.2, 1.0),  # Change for datasets with lots of features\n",
    "    'reg_alpha' : (0.1,10),\n",
    "    'reg_lambda' : (0.1,10),\n",
    "    }\n",
    "\n",
    "optimizerXGB = BayesianOptimization(\n",
    "    f=XGBClf_bayes_opt,\n",
    "    pbounds=pbounds_XGB,\n",
    "    random_state=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerXGB.maximize(n_iter = 50,\n",
    "                      init_points = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestRun = optimizerXGB.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIT THE MODEL WITH THE BEST HYPERPARAMETERS\n",
    "xgb_grid_params = {'clf__learning_rate': bestRun['params']['learning_rate'] ,\n",
    "                    'clf__subsample'    : bestRun['params']['subsample']  ,\n",
    "                    'clf__n_estimators' : int(bestRun['params']['n_estimators'])  , \n",
    "                    'clf__scale_pos_weight' : bestRun['params']['scale_pw'],\n",
    "                    'clf__reg_alpha': bestRun['params']['reg_alpha'] ,\n",
    "                    'clf__reg_lambda': bestRun['params']['reg_lambda']  ,\n",
    "                    'clf__max_depth'    : int(bestRun['params']['max_depth']) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(np.cumsum(PCApipe['dim_reduc'].explained_variance_ratio_))),np.cumsum(PCApipe['dim_reduc'].explained_variance_ratio_))\n",
    "plt.title('Varianza explicada acumulada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_parquet(\"C:/Users/jf95n/OneDrive/Desktop/KaggleCompetition/df_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_submission2 = clone(final_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_submission2.set_params(**xgb_grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_submission2.fit(full_data_2.loc[:,num_idx + categ_idx],full_data_2.loc[:,target_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['Target'] = pipe_submission2.predict(df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.loc[:,['ID','Target']].to_csv('xgbClfBalanced_250324.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0cba59e8686724c8b06d06e1b035ecd6727a6f1ef058d75f8ca5298c8c61292"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
